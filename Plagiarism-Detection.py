# -*- coding: utf-8 -*-
"""mine.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MXfiTG3Z8LORnmnXTr0LAJK4yY0hFI5E
"""

# Install Java
!apt-get install openjdk-11-jdk-headless -qq > /dev/null

# Download Spark
!wget -q https://archive.apache.org/dist/spark/spark-3.4.1/spark-3.4.1-bin-hadoop3.tgz

# Extract Spark
!tar -xvzf spark-3.4.1-bin-hadoop3.tgz

# Install Spark NLP and findspark
!pip install -q findspark spark-nlp

# Set environment variables
import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-11-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-3.4.1-bin-hadoop3"

import findspark
findspark.init()

# âœ… Start Spark NLP session (IMPORTANT!)
import sparknlp
spark = sparknlp.start()
spark

spark

from google.colab import files
uploaded = files.upload()

file_path = '/content/user_input.txt'

df = spark.read.csv('/content/user_input.txt', header=True, inferSchema=True)
df.show(5)

!pip install PyMuPDF

import fitz  # PyMuPDF

pdf_file_path = '/content/user_input.txt'
pdf_document = fitz.open(pdf_file_path)


pdf_text = ""
for page_num in range(pdf_document.page_count):
    page = pdf_document.load_page(page_num)
    pdf_text += page.get_text()


print(pdf_text[:500])

from pyspark.sql import SparkSession
spark = SparkSession.builder \
    .appName("PlagiarismDetection") \
    .getOrCreate()

from pyspark.sql import Row


data = [Row(text=pdf_text)]
df = spark.createDataFrame(data)


df.show(truncate=False)

from pyspark.sql import SparkSession
from pyspark.sql.functions import lit


spark = SparkSession.builder \
    .appName("PlagiarismDetection") \
    .getOrCreate()


data = [(pdf_text,)]
columns = ["text"]


df = spark.createDataFrame(data, columns)



from sparknlp.base import DocumentAssembler
from sparknlp.annotator import Tokenizer, StopWordsCleaner, LemmatizerModel
from pyspark.ml import Pipeline

document_assembler = DocumentAssembler().setInputCol("text").setOutputCol("document")
tokenizer = Tokenizer().setInputCols(["document"]).setOutputCol("token")
stopwords_cleaner = StopWordsCleaner().setInputCols(["token"]).setOutputCol("clean_tokens")
lemmatizer = LemmatizerModel.pretrained().setInputCols(["clean_tokens"]).setOutputCol("lemmatized")

# Build the pipeline
pipeline = Pipeline(stages=[document_assembler, tokenizer, stopwords_cleaner, lemmatizer])

# Step 3: Apply the pipeline to the DataFrame
processed_data = pipeline.fit(df).transform(df)

# Step 4: Show the preprocessed data (lemmatized tokens)
processed_data.select("lemmatized.result").show(truncate=False)

from pyspark.sql.functions import col

# Extract only the 'result' from the 'lemmatized' column
processed_data = processed_data.withColumn("lemmatized_tokens",
                                           col("lemmatized.result"))

# Import necessary classes
from pyspark.ml.feature import HashingTF, IDF

# Generate term frequency (TF) features
hashing_tf = HashingTF(inputCol="lemmatized_tokens", outputCol="raw_features")
featurized_data = hashing_tf.transform(processed_data)

# Apply IDF (Inverse Document Frequency)
idf = IDF(inputCol="raw_features", outputCol="features")
idf_model = idf.fit(featurized_data)
tfidf_data = idf_model.transform(featurized_data)

# Show the features (TF-IDF values)
tfidf_data.select("features").show(truncate=False)

from pyspark.ml.feature import NGram

# Generate bigrams (n=2)
ngram = NGram(n=2, inputCol="lemmatized_tokens", outputCol="bigrams")
ngram_data = ngram.transform(processed_data)

# Show the bigrams
ngram_data.select("bigrams").show(truncate=False)

# Save the processed data to a Parquet file and overwrite if it exists
processed_data.write.mode("overwrite").format("parquet").save("/content/processed_data.parquet")

!pip install beautifulsoup4
!pip install requests

!pip install pymongo

!pip install apache-airflow

!pip install kafka-python

import nltk
nltk.download('stopwords')

import requests
from bs4 import BeautifulSoup
import json
import string
from nltk.corpus import stopwords
from collections import Counter

# --- STEP 1: Read input document ---
def read_text_file(file_path):
    with open(file_path, 'r', encoding='utf-8') as f:
        return f.read()

# --- STEP 2: Extract keyword-based query ---
def generate_query(text, num_keywords=5):
    text = text.lower().translate(str.maketrans('', '', string.punctuation))
    words = text.split()
    stop_words = set(stopwords.words('english'))
    filtered_words = [w for w in words if w not in stop_words]
    most_common = Counter(filtered_words).most_common(num_keywords)
    keywords = [word for word, count in most_common]
    return " ".join(keywords)

# --- STEP 3: Fetch articles from arXiv ---
def fetch_arxiv_articles(query, max_results=5):
    search_url = f'http://export.arxiv.org/api/query?search_query=all:{query}&start=0&max_results={max_results}'
    response = requests.get(search_url)
    response.raise_for_status()
    soup = BeautifulSoup(response.content, 'xml')
    articles = []
    for entry in soup.find_all('entry'):
        title = entry.title.text
        authors = [author.find('name').text for author in entry.find_all('author')]
        abstract = entry.summary.text
        url = entry.id.text
        articles.append({
            'title': title,
            'authors': authors,
            'abstract': abstract,
            'url': url
        })
    return articles

# --- MAIN ---
if __name__ == "__main__":
    file_path = "user_input.txt"  # Replace with your file path
    doc_text = read_text_file(file_path)

    generated_query = generate_query(doc_text)
    print(f"ðŸ” Generated Query: {generated_query}")

    print("\nðŸ“„ Fetching Similar Articles from arXiv...\n")
    arxiv_results = fetch_arxiv_articles(generated_query)

    for idx, article in enumerate(arxiv_results, 1):
        print(f"Article {idx}:")
        print(json.dumps(article, indent=2))
        print("-" * 40)

import nltk
nltk.download('stopwords')

with open("/content/user_input.txt", 'r', encoding='utf-8') as f:
    pdf_text = f.read()

import requests
from bs4 import BeautifulSoup
import json
import string
from nltk.corpus import stopwords
from collections import Counter

# --- STEP 1: Read input document ---
def read_text_file(file_path):
    with open(file_path, 'r', encoding='utf-8') as f:
        return f.read()

# --- STEP 2: Extract keyword-based query ---
def generate_query(text, num_keywords=5):
    text = text.lower().translate(str.maketrans('', '', string.punctuation))
    words = text.split()
    stop_words = set(stopwords.words('english'))
    filtered_words = [w for w in words if w not in stop_words]
    most_common = Counter(filtered_words).most_common(num_keywords)
    keywords = [word for word, count in most_common]
    return " ".join(keywords)

# --- STEP 3: Fetch articles from arXiv ---
def fetch_arxiv_articles(query, max_results=5):
    search_url = f'http://export.arxiv.org/api/query?search_query=all:{query}&start=0&max_results={max_results}'
    response = requests.get(search_url)
    response.raise_for_status()
    soup = BeautifulSoup(response.content, 'xml')
    articles = []
    for entry in soup.find_all('entry'):
        title = entry.title.text
        authors = [author.find('name').text for author in entry.find_all('author')]
        abstract = entry.summary.text
        url = entry.id.text
        articles.append({
            'title': title,
            'authors': authors,
            'abstract': abstract,
            'url': url
        })
    return articles

# --- MAIN ---
if __name__ == "__main__":
    file_path = "user_input.txt"  # Replace with your file path
    doc_text = read_text_file(file_path)

    generated_query = generate_query(doc_text)
    print(f"ðŸ” Generated Query: {generated_query}")

    print("\nðŸ“„ Fetching Similar Articles from arXiv...\n")
    arxiv_results = fetch_arxiv_articles(generated_query)

    # Save the articles data as a JSON file
    with open('/content/web_dataset.json', 'w') as f:
        json.dump(arxiv_results, f, indent=4)  # Use indent for better formatting

    for idx, article in enumerate(arxiv_results, 1):
        print(f"Article {idx}:")
        print(json.dumps(article, indent=2))
        print("-" * 40)

from pyspark.sql import Row
from pyspark.sql.functions import lit
from pyspark.ml.feature import Tokenizer, HashingTF, IDF
from pyspark.ml.linalg import Vectors
from pyspark.ml.feature import Normalizer
from pyspark.ml.linalg import DenseVector
from pyspark.sql.functions import udf
from pyspark.sql.types import DoubleType
import math

# Sample assumption: user_text already exists from PDF preprocessing
# If not, set user_text = "Your extracted document text"

# Step 2.1: Prepare the web data from arXiv (if already fetched)
# If you have arxiv_results from earlier:
arxiv_df = spark.createDataFrame(
    [Row(text=article["title"] + ". " + article["abstract"], source="arxiv") for article in arxiv_results]
)

# Step 2.2: Convert user text into a DataFrame
user_df = spark.createDataFrame([Row(text=pdf_text, source="user_input")])

# Step 2.3: Combine both into combined_df
combined_df = user_df.union(arxiv_df)

# Step 2.4: Tokenization
tokenizer = Tokenizer(inputCol="text", outputCol="words")
words_data = tokenizer.transform(combined_df)

# Step 2.5: TF and IDF
hashingTF = HashingTF(inputCol="words", outputCol="rawFeatures", numFeatures=10000)
featurized_data = hashingTF.transform(words_data)

idf = IDF(inputCol="rawFeatures", outputCol="features")
idf_model = idf.fit(featurized_data)
rescaled_data = idf_model.transform(featurized_data)

# Step 2.6: Normalize vectors for cosine similarity
normalizer = Normalizer(inputCol="features", outputCol="normFeatures")
normalized_data = normalizer.transform(rescaled_data)

# Step 2.7: Compute Cosine Similarity of user_input with each arxiv result
user_vector = normalized_data.filter(normalized_data.source == "user_input").select("normFeatures").first()["normFeatures"]

# UDF to compute cosine similarity
def cosine_sim(vec):
    return float(vec.dot(user_vector))

cosine_sim_udf = udf(cosine_sim, DoubleType())
scored_df = normalized_data.withColumn("cosine_similarity", cosine_sim_udf("normFeatures"))

# Step 2.8: Show sorted results (excluding self-comparison)
scored_df.filter(scored_df.source != "user_input") \
    .select("text", "cosine_similarity") \
    .orderBy("cosine_similarity", ascending=False) \
    .show(truncate=False)

!pip install nltk

# STEP 0: INSTALLS & SETUP
!apt-get install openjdk-11-jdk-headless -qq > /dev/null
!wget -q https://archive.apache.org/dist/spark/spark-3.4.1/spark-3.4.1-bin-hadoop3.tgz
!tar -xvzf spark-3.4.1-bin-hadoop3.tgz
!pip install -q findspark spark-nlp nltk beautifulsoup4 requests sentence-transformers

import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-11-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-3.4.1-bin-hadoop3"

import findspark
findspark.init()

import nltk
nltk.download("punkt")
nltk.download("stopwords")

import requests, string, json
from bs4 import BeautifulSoup
from collections import Counter
from nltk.corpus import stopwords
from nltk.tokenize import sent_tokenize
from sentence_transformers import SentenceTransformer, util
from pyspark.sql import SparkSession, Row
from pyspark.sql.functions import col, lit
from pyspark.ml.feature import Tokenizer, HashingTF, IDF
import pandas as pd

spark = SparkSession.builder.appName("PlagiarismDetectionPipeline").getOrCreate()

# STEP 1: USER FILE UPLOAD
from google.colab import files
uploaded = files.upload()
file_path = list(uploaded.keys())[0]

# STEP 2: READ TEXT FILE + EXTRACT KEYWORDS
def read_text_file(fp):
    with open(fp, 'r', encoding='utf-8') as f:
        return f.read()

def generate_query(text, num_keywords=5):
    text = text.lower().translate(str.maketrans('', '', string.punctuation))
    words = text.split()
    stop_words = set(stopwords.words('english'))
    filtered = [w for w in words if w not in stop_words]
    keywords = [word for word, count in Counter(filtered).most_common(num_keywords)]
    return " ".join(keywords)

user_text = read_text_file(file_path)
query = generate_query(user_text)

# STEP 3: FETCH SIMILAR ARTICLES FROM ARXIV
def fetch_arxiv_articles(query, max_results=5):
    search_url = f"http://export.arxiv.org/api/query?search_query=all:{query}&start=0&max_results={max_results}"
    response = requests.get(search_url)
    soup = BeautifulSoup(response.content, 'xml')
    articles = []
    for entry in soup.find_all('entry'):
        title = entry.title.text
        authors = [author.find('name').text for author in entry.find_all('author')]
        abstract = entry.summary.text
        url = entry.id.text
        articles.append(f"{title}. {abstract}")
    return articles

arxiv_articles = fetch_arxiv_articles(query)
all_docs = [user_text] + arxiv_articles

# STEP 4: LOAD INTO SPARK
rows = [Row(text=doc) for doc in all_docs]
df = spark.createDataFrame(rows)

# STEP 5: TOKENIZATION + TF-IDF
tokenizer = Tokenizer(inputCol="text", outputCol="words")
words_data = tokenizer.transform(df)
hashing_tf = HashingTF(inputCol="words", outputCol="raw_features")
featurized_data = hashing_tf.transform(words_data)
idf = IDF(inputCol="raw_features", outputCol="features")
idf_model = idf.fit(featurized_data)
tfidf_data = idf_model.transform(featurized_data)

# STEP 6: COSINE SIMILARITY + SENTENCE MATCHING
model = SentenceTransformer('all-MiniLM-L6-v2')

user_sentences = sent_tokenize(user_text)
article_sentences = []
article_ids = []

for i, article in enumerate(arxiv_articles):
    sents = sent_tokenize(article)
    article_sentences.extend(sents)
    article_ids.extend([i + 1] * len(sents))  # i+1 because 0 is the user

user_embeddings = model.encode(user_sentences, convert_to_tensor=True)
article_embeddings = model.encode(article_sentences, convert_to_tensor=True)

cos_scores = util.cos_sim(user_embeddings, article_embeddings)

similar_pairs = []
for i in range(len(user_sentences)):
    for j in range(len(article_sentences)):
        score = cos_scores[i][j].item()
        if score > 0.75:
            similar_pairs.append({
                "user_sentence": user_sentences[i],
                "matched_sentence": article_sentences[j],
                "article_id": article_ids[j],
                "similarity_score": score
            })

# STEP 7: DISPLAY TOP MATCHES
if similar_pairs:
    df_result = pd.DataFrame(similar_pairs).sort_values(by="similarity_score", ascending=False)
    print("\nðŸ§  Top Similar Sentences Found:")
    display(df_result.head(10))
else:
    print("âœ… No significant plagiarism detected.")

# OPTIONAL STEP 8: SAVE TO JSON REPORT
with open("/content/plagiarism_report.json", "w") as f:
    json.dump(similar_pairs, f, indent=2)

# Install dependencies for PDF generation
!apt-get install -y wkhtmltopdf
!pip install pdfkit

# Install Gradio for Web UI
!pip install gradio

from google.colab import files

# Upload the document
uploaded = files.upload()

# Read the document content
file_path = list(uploaded.keys())[0]  # Get the filename
with open(file_path, 'r') as file:
    user_document = file.read()

print("Document successfully uploaded and read!")

import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

# Download NLTK resources
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('punkt_tab') # Download the missing Punkt model

# Tokenize and remove stopwords
def preprocess_text(text):
    stop_words = set(stopwords.words('english'))
    word_tokens = word_tokenize(text.lower())  # Tokenize and convert to lowercase
    filtered_tokens = [word for word in word_tokens if word.isalnum() and word not in stop_words]
    return " ".join(filtered_tokens)

processed_document = preprocess_text(user_document)
print("Preprocessing completed!")

import requests
from bs4 import BeautifulSoup

# Fetch articles from ArXiv based on a query
def fetch_arxiv_articles(query, max_results=5):
    search_url = f'http://export.arxiv.org/api/query?search_query=all:{query}&start=0&max_results={max_results}'
    response = requests.get(search_url)
    response.raise_for_status()
    soup = BeautifulSoup(response.content, 'xml')
    articles = []
    for entry in soup.find_all('entry'):
        title = entry.title.text
        authors = [author.find('name').text for author in entry.find_all('author')]
        abstract = entry.summary.text
        url = entry.id.text
        articles.append({'title': title, 'authors': authors, 'abstract': abstract, 'url': url})
    return articles

# Generate query from the document text
query = ' '.join(processed_document.split()[:5])  # First 5 words of the document as the query
articles = fetch_arxiv_articles(query)
print(f"Found {len(articles)} articles!")

from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity

# Initialize sentence transformer model (BERT-based)
model = SentenceTransformer('all-MiniLM-L6-v2')

# Encode the user's document and fetched articles
doc_embedding = model.encode([processed_document])
article_embeddings = model.encode([article['abstract'] for article in articles])

# Compute cosine similarity between document and articles
similarity_scores = cosine_similarity(doc_embedding, article_embeddings)

# Show the similarity scores
for idx, score in enumerate(similarity_scores[0]):
    print(f"Article {idx+1}: Similarity Score = {score:.4f}")

!pip install fpdf

!pip install reportlab

from reportlab.lib.pagesizes import letter
from reportlab.pdfgen import canvas
import datetime

# Generate report using reportlab
def generate_report_with_reportlab(similarity_scores, articles, user_document):
    pdf_output_path = "/content/plagiarism_report_reportlab.pdf"

    c = canvas.Canvas(pdf_output_path, pagesize=letter)

    # Title
    c.setFont("Helvetica-Bold", 16)
    c.drawString(200, 750, "Plagiarism Detection Report")

    # Date
    c.setFont("Helvetica", 12)
    c.drawString(200, 730, f"Date: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

    # User document preview
    c.drawString(50, 700, "User Document Preview:")
    c.setFont("Helvetica", 10)
    c.drawString(50, 680, f"{user_document[:500]}...")  # Preview first 500 characters

    # Articles and similarity results
    y_position = 650
    c.setFont("Helvetica-Bold", 12)
    for idx, score in enumerate(similarity_scores[0]):
        c.drawString(50, y_position, f"Article {idx+1}: Similarity Score = {score:.4f}")
        y_position -= 20
        c.setFont("Helvetica", 10)
        c.drawString(50, y_position, f"Title: {articles[idx]['title']}")
        y_position -= 20
        c.drawString(50, y_position, f"Abstract: {articles[idx]['abstract']}")
        y_position -= 20
        c.drawString(50, y_position, f"URL: {articles[idx]['url']}")
        y_position -= 40

    c.save()
    return pdf_output_path

# Generate the report with reportlab
pdf_report_path_rl = generate_report_with_reportlab(similarity_scores, articles, user_document)
print(f"Plagiarism report saved to: {pdf_report_path_rl}")

pdf_report_path_rl = generate_report_with_reportlab(similarity_scores, articles, user_document)
print(f"Plagiarism report saved to: {pdf_report_path_rl}")

from google.colab import files
files.download(pdf_report_path_rl)  # For ReportLab

!pip install Flask Flask-Uploads

!pip install flask-ngrok

